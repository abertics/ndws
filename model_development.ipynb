{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b52c3b88-9807-462d-980a-68fb74fee7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from e2cnn import gspaces\n",
    "from e2cnn import nn as enn\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tfrecord.torch.dataset import MultiTFRecordDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from os import path as osp\n",
    "\n",
    "from torch.utils.data import IterableDataset\n",
    "from fuel_embedding.utils import load\n",
    "import matplotlib.pyplot as plt\n",
    "import deepdish as dd\n",
    "\n",
    "from trainer import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ef69b7ea-7023-438c-9d74-5dedcace0b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AACN_Layer(nn.Module):\n",
    "    def __init__(self, in_channels,out_channels, k=0.25, v=0.25, kernel_size=3, num_heads=8, image_size=224, inference=False):\n",
    "        super(AACN_Layer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dk = math.floor((in_channels * k) / num_heads) * num_heads\n",
    "        # Ensure a minimum of 20 dimensions per head for the keys\n",
    "        if self.dk / num_heads < 20:\n",
    "            self.dk = num_heads * 20\n",
    "        self.dv = math.floor((in_channels * v) / num_heads) * num_heads\n",
    "\n",
    "        assert self.dk % self.num_heads == 0, \"dk should be divisible by num_heads.\"\n",
    "        assert self.dv % self.num_heads == 0, \"dv should be divisible by num_heads.\"\n",
    "\n",
    "        self.padding = (self.kernel_size - 1) // 2\n",
    "\n",
    "        # Modify conv_out to output 1 channel\n",
    "        self.conv_out = nn.Conv2d(self.in_channels, out_channels, self.kernel_size, padding=self.padding)\n",
    "        # Adjust kqv_conv accordingly\n",
    "        self.kqv_conv = nn.Conv2d(self.in_channels, 2 * self.dk + self.dv, kernel_size=1)\n",
    "        # Modify attn_out to output 1 channel\n",
    "        self.attn_out = nn.Conv2d(self.dv, out_channels, 1)\n",
    "\n",
    "        # Positional encodings\n",
    "        self.rel_encoding_h = nn.Parameter(\n",
    "            torch.randn((2 * image_size - 1, self.dk // self.num_heads), requires_grad=True)\n",
    "        )\n",
    "        self.rel_encoding_w = nn.Parameter(\n",
    "            torch.randn((2 * image_size - 1, self.dk // self.num_heads), requires_grad=True)\n",
    "        )\n",
    "\n",
    "        # Optionally store attention weights\n",
    "        self.inference = inference\n",
    "        if self.inference:\n",
    "            self.register_parameter('weights', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, height, width = x.size()\n",
    "        dkh = self.dk // self.num_heads\n",
    "        dvh = self.dv // self.num_heads\n",
    "        flatten_hw = lambda x, depth: torch.reshape(x, (batch_size, self.num_heads, height * width, depth))\n",
    "\n",
    "        # Compute q, k, v\n",
    "        kqv = self.kqv_conv(x)\n",
    "        k, q, v = torch.split(kqv, [self.dk, self.dk, self.dv], dim=1)\n",
    "        q = q * (dkh ** -0.5)\n",
    "\n",
    "        # Split heads\n",
    "        k = self.split_heads_2d(k, self.num_heads)\n",
    "        q = self.split_heads_2d(q, self.num_heads)\n",
    "        v = self.split_heads_2d(v, self.num_heads)\n",
    "\n",
    "        # Compute attention logits\n",
    "        qk = torch.matmul(flatten_hw(q, dkh), flatten_hw(k, dkh).transpose(2, 3))\n",
    "\n",
    "        # Add relative logits\n",
    "        qr_h, qr_w = self.relative_logits(q)\n",
    "        qk += qr_h\n",
    "        qk += qr_w\n",
    "\n",
    "        # Compute attention weights\n",
    "        weights = F.softmax(qk, dim=-1)\n",
    "        if self.inference:\n",
    "            self.weights = nn.Parameter(weights)\n",
    "\n",
    "        # Compute attention output\n",
    "        attn_out = torch.matmul(weights, flatten_hw(v, dvh))\n",
    "        attn_out = torch.reshape(attn_out, (batch_size, self.num_heads, dvh, height, width))\n",
    "        attn_out = self.combine_heads_2d(attn_out)\n",
    "        attn_out = self.attn_out(attn_out)\n",
    "\n",
    "        # Compute conv_out\n",
    "        conv_out = self.conv_out(x)\n",
    "\n",
    "        # Sum conv_out and attn_out to produce output of shape (B, 1, H, W)\n",
    "        return conv_out + attn_out\n",
    "\n",
    "    # Split channels into multiple heads\n",
    "    def split_heads_2d(self, inputs, num_heads):\n",
    "        batch_size, depth, height, width = inputs.size()\n",
    "        ret_shape = (batch_size, num_heads, height, width, depth // num_heads)\n",
    "        split_inputs = torch.reshape(inputs, ret_shape)\n",
    "        return split_inputs\n",
    "\n",
    "    # Combine heads (inverse of split_heads_2d)\n",
    "    def combine_heads_2d(self, inputs):\n",
    "        batch_size, num_heads, depth, height, width = inputs.size()\n",
    "        ret_shape = (batch_size, num_heads * depth, height, width)\n",
    "        return torch.reshape(inputs, ret_shape)\n",
    "\n",
    "    # Compute relative logits for both dimensions\n",
    "    def relative_logits(self, q):\n",
    "        _, num_heads, height, width, dkh = q.size()\n",
    "        rel_logits_w = self.relative_logits_1d(\n",
    "            q, self.rel_encoding_w, height, width, num_heads, [0, 1, 2, 4, 3, 5]\n",
    "        )\n",
    "        rel_logits_h = self.relative_logits_1d(\n",
    "            torch.transpose(q, 2, 3), self.rel_encoding_h, width, height, num_heads, [0, 1, 4, 2, 5, 3]\n",
    "        )\n",
    "        return rel_logits_h, rel_logits_w\n",
    "\n",
    "    # Compute relative logits along one dimension\n",
    "    def relative_logits_1d(self, q, rel_k, height, width, num_heads, transpose_mask):\n",
    "        rel_logits = torch.einsum('bhxyd,md->bxym', q, rel_k)\n",
    "        # Collapse height and heads\n",
    "        rel_logits = torch.reshape(rel_logits, (-1, height, width, 2 * width - 1))\n",
    "        rel_logits = self.rel_to_abs(rel_logits)\n",
    "        # Shape it\n",
    "        rel_logits = torch.reshape(rel_logits, (-1, height, width, width))\n",
    "        # Tile for each head\n",
    "        rel_logits = torch.unsqueeze(rel_logits, dim=1)\n",
    "        rel_logits = rel_logits.repeat(1, num_heads, 1, 1, 1)\n",
    "        # Tile height / width times\n",
    "        rel_logits = torch.unsqueeze(rel_logits, dim=3)\n",
    "        rel_logits = rel_logits.repeat(1, 1, 1, height, 1, 1)\n",
    "        # Reshape for adding to the logits\n",
    "        rel_logits = rel_logits.permute(transpose_mask)\n",
    "        rel_logits = torch.reshape(rel_logits, (-1, num_heads, height * width, height * width))\n",
    "        return rel_logits\n",
    "\n",
    "    # Converts tensor from relative to absolute indexing\n",
    "    def rel_to_abs(self, x):\n",
    "        batch_size, num_heads, L, _ = x.size()\n",
    "        # Pad to shift from relative to absolute indexing\n",
    "        col_pad = torch.zeros((batch_size, num_heads, L, 1), device=x.device)\n",
    "        x = torch.cat((x, col_pad), dim=3)\n",
    "        flat_x = torch.reshape(x, (batch_size, num_heads, L * 2 * L))\n",
    "        flat_pad = torch.zeros((batch_size, num_heads, L - 1), device=x.device)\n",
    "        flat_x_padded = torch.cat((flat_x, flat_pad), dim=2)\n",
    "        # Reshape and slice out the padded elements\n",
    "        final_x = torch.reshape(flat_x_padded, (batch_size, num_heads, L + 1, 2 * L - 1))\n",
    "        final_x = final_x[:, :, :L, L - 1 :]\n",
    "        return final_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "541d5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquivariantResidualBlock(nn.Module):\n",
    "    def __init__(self, in_type, out_type, dropout_prob):\n",
    "        super(EquivariantResidualBlock, self).__init__()\n",
    "        self.conv1 = enn.R2Conv(in_type, out_type, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = enn.InnerBatchNorm(out_type)\n",
    "        self.relu = enn.ReLU(out_type, inplace=True)\n",
    "        self.dropout = enn.PointwiseDropout(out_type, p=dropout_prob)\n",
    "        self.conv2 = enn.R2Conv(out_type, out_type, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = enn.InnerBatchNorm(out_type)\n",
    "\n",
    "        # If in_type and out_type are different, use a projection\n",
    "        if in_type != out_type:\n",
    "            self.downsample = enn.R2Conv(in_type, out_type, kernel_size=1, bias=False)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "\n",
    "        out = out + identity  # Element-wise addition of GeometricTensors\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b7ca8adc-d24a-4d6a-87d4-e8a8a088f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquivariantVectorEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, conv_channels, dropout_prob):\n",
    "        super(EquivariantVectorEncoder, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_channels = conv_channels\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Define the rotational symmetry group\n",
    "        self.r2_act = gspaces.Rot2dOnR2(N=8)  # N=8 for 8-fold rotational symmetry\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "        prev_type = enn.FieldType(self.r2_act, in_channels * [self.r2_act.trivial_repr])\n",
    "        self.encoder_types = []\n",
    "        for out_channels in conv_channels:\n",
    "            out_type = enn.FieldType(self.r2_act, out_channels * [self.r2_act.regular_repr])\n",
    "            self.encoder_types.append(out_type)\n",
    "            self.encoder_layers.append(\n",
    "                nn.Sequential(\n",
    "                    EquivariantResidualBlock(prev_type, out_type, dropout_prob),\n",
    "                    enn.GroupPooling(out_type)\n",
    "                )\n",
    "            )\n",
    "            prev_type = enn.FieldType(self.r2_act, out_channels * [self.r2_act.trivial_repr])\n",
    "            self.encoder_layers.append(enn.PointwiseMaxPool(prev_type, kernel_size=2, stride=2))\n",
    "\n",
    "        self.embed_dim = prev_type.size  # Number of channels after encoding\n",
    "        \n",
    "        # Add global average pooling to get a vector representation\n",
    "        self.global_pool = enn.GroupPooling(prev_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure x is a torch.Tensor with the correct dtype and device\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        elif x.dtype != torch.float32:\n",
    "            x = x.float()\n",
    "        \n",
    "        # Wrap input in GeometricTensor\n",
    "        input_type = enn.FieldType(self.r2_act, self.in_channels * [self.r2_act.trivial_repr])\n",
    "        x = enn.GeometricTensor(x, input_type)\n",
    "\n",
    "        # Encoder\n",
    "        print('before encoder',x.shape)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "            print(x.shape)\n",
    "            \n",
    "        # Final pooling to reduce spatial dimensions\n",
    "        x = F.avg_pool2d(x.tensor, kernel_size=2, stride=2)\n",
    "        \n",
    "        return x  # Return flattened vector [B, C*H*W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b0190061-849e-40b3-ab54-8686ce750b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquivariantVectorDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, conv_channels, dropout_prob):\n",
    "        super(EquivariantVectorDecoder, self).__init__()\n",
    "        self.in_channels = in_channels  # Input channels from latent representation\n",
    "        self.out_channels = out_channels  # Number of channels in output image\n",
    "        self.conv_channels = conv_channels\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Define the rotational symmetry group\n",
    "        self.r2_act = gspaces.Rot2dOnR2(N=8)  # N=8 for 8-fold rotational symmetry\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_layers = nn.ModuleList()\n",
    "        prev_type = enn.FieldType(self.r2_act, self.in_channels * [self.r2_act.trivial_repr])\n",
    "        self.decoder_types = []\n",
    "        for out_channels in reversed(conv_channels):\n",
    "            out_type = enn.FieldType(self.r2_act, out_channels * [self.r2_act.regular_repr])\n",
    "            self.decoder_types.append(out_type)\n",
    "            self.decoder_layers.append(\n",
    "                nn.Sequential(\n",
    "                    # Upsample\n",
    "                    enn.R2Upsampling(prev_type, scale_factor=2, mode='bilinear'),\n",
    "                    # Equivariant Residual Block\n",
    "                    EquivariantResidualBlock(prev_type, out_type, self.dropout_prob)\n",
    "                )\n",
    "            )\n",
    "            prev_type = out_type  # Update prev_type for the next layer\n",
    "\n",
    "        # Final layer to map to desired output channels\n",
    "        final_type = enn.FieldType(self.r2_act, self.out_channels * [self.r2_act.trivial_repr])\n",
    "        self.final_layer = enn.R2Conv(prev_type, final_type, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure x is a torch.Tensor with the correct dtype and device\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32, device=x.device)\n",
    "        elif x.dtype != torch.float32:\n",
    "            x = x.float()\n",
    "\n",
    "        # Wrap input in GeometricTensor\n",
    "        input_type = enn.FieldType(self.r2_act, self.in_channels * [self.r2_act.trivial_repr])\n",
    "        x = enn.GeometricTensor(x, input_type)\n",
    "\n",
    "        # Decoder\n",
    "        print('before decoder', x.shape)\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x)\n",
    "            print(x.shape)\n",
    "\n",
    "        x = self.final_layer(x)\n",
    "\n",
    "        return x.tensor  # Return output image tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "06c79dcb-8b7f-4b9f-a317-cdcce097b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from einops import rearrange\n",
    "\n",
    "class MultiScaleGroupTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        group,\n",
    "        in_channels=3,\n",
    "        base_channels=128,\n",
    "        num_heads=8,\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        dropout_rate=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder path with increasing channels\n",
    "        self.encoder1 = GroupTransformer(\n",
    "            group=group,\n",
    "            in_channels=in_channels,\n",
    "            num_channels=base_channels,\n",
    "            num_heads=num_heads,\n",
    "            block_sizes=[2, 2],\n",
    "            expansion_per_block=[1, 1],\n",
    "            crop_per_layer=0,\n",
    "            normalize_between_layers=True,\n",
    "            maxpool_after_last_block=True,\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            activation_function=\"Swish\",\n",
    "            attention_dropout_rate=dropout_rate,\n",
    "            value_dropout_rate=dropout_rate,\n",
    "            input_dropout_rate=dropout_rate,\n",
    "            num_classes=base_channels,\n",
    "        )\n",
    "        \n",
    "        self.encoder2 = GroupTransformer(\n",
    "            group=group,\n",
    "            in_channels=base_channels,\n",
    "            num_channels=base_channels * 2,\n",
    "            num_heads=num_heads * 2,\n",
    "            block_sizes=[2, 2],\n",
    "            expansion_per_block=[1, 1],\n",
    "            crop_per_layer=0,\n",
    "            normalize_between_layers=True,\n",
    "            maxpool_after_last_block=True,\n",
    "            image_size=image_size // 2,\n",
    "            patch_size=patch_size // 2,\n",
    "            activation_function=\"Swish\",\n",
    "            attention_dropout_rate=dropout_rate,\n",
    "            value_dropout_rate=dropout_rate,\n",
    "            input_dropout_rate=dropout_rate,\n",
    "            num_classes=base_channels * 2,\n",
    "        )\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = GroupTransformer(\n",
    "            group=group,\n",
    "            in_channels=base_channels * 2,\n",
    "            num_channels=base_channels * 4,\n",
    "            num_heads=num_heads * 4,\n",
    "            block_sizes=[6],\n",
    "            expansion_per_block=[1],\n",
    "            crop_per_layer=0,\n",
    "            normalize_between_layers=True,\n",
    "            maxpool_after_last_block=False,\n",
    "            image_size=image_size // 4,\n",
    "            patch_size=patch_size // 4,\n",
    "            activation_function=\"Swish\",\n",
    "            attention_dropout_rate=dropout_rate,\n",
    "            value_dropout_rate=dropout_rate,\n",
    "            input_dropout_rate=dropout_rate,\n",
    "            num_classes=base_channels * 4,\n",
    "        )\n",
    "        \n",
    "        # Decoder path with decreasing channels\n",
    "        self.decoder1 = GroupTransformer(\n",
    "            group=group,\n",
    "            in_channels=base_channels * 6,  # Concatenated with skip connection\n",
    "            num_channels=base_channels * 2,\n",
    "            num_heads=num_heads * 2,\n",
    "            block_sizes=[2, 2],\n",
    "            expansion_per_block=[1, 1],\n",
    "            crop_per_layer=0,\n",
    "            normalize_between_layers=True,\n",
    "            maxpool_after_last_block=False,\n",
    "            image_size=image_size // 2,\n",
    "            patch_size=patch_size // 2,\n",
    "            activation_function=\"Swish\",\n",
    "            attention_dropout_rate=dropout_rate,\n",
    "            value_dropout_rate=dropout_rate,\n",
    "            input_dropout_rate=dropout_rate,\n",
    "            num_classes=base_channels * 2,\n",
    "        )\n",
    "        \n",
    "        self.decoder2 = GroupTransformer(\n",
    "            group=group,\n",
    "            in_channels=base_channels * 3,  # Concatenated with skip connection\n",
    "            num_channels=base_channels,\n",
    "            num_heads=num_heads,\n",
    "            block_sizes=[2, 2],\n",
    "            expansion_per_block=[1, 1],\n",
    "            crop_per_layer=0,\n",
    "            normalize_between_layers=True,\n",
    "            maxpool_after_last_block=False,\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            activation_function=\"Swish\",\n",
    "            attention_dropout_rate=dropout_rate,\n",
    "            value_dropout_rate=dropout_rate,\n",
    "            input_dropout_rate=dropout_rate,\n",
    "            num_classes=1,  # Final binary output\n",
    "        )\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.reshape_for_next_layer(e1))\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.reshape_for_next_layer(e2))\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d1 = self.decoder1(torch.cat([\n",
    "            self.upsample(self.reshape_for_next_layer(b)), \n",
    "            self.reshape_for_next_layer(e2)\n",
    "        ], dim=1))\n",
    "        \n",
    "        d2 = self.decoder2(torch.cat([\n",
    "            self.upsample(self.reshape_for_next_layer(d1)), \n",
    "            self.reshape_for_next_layer(e1)\n",
    "        ], dim=1))\n",
    "        \n",
    "        return d2\n",
    "    \n",
    "    def reshape_for_next_layer(self, x):\n",
    "        # Reshape transformer output for next layer\n",
    "        if len(x.shape) == 2:  # B, C\n",
    "            B, C = x.shape\n",
    "            H = W = int(C ** 0.5)\n",
    "            return x.view(B, C, 1, H, W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "28d1f72b-76af-4098-9866-a2b94aa5fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(128,15,32,32)\n",
    "\n",
    "model = EquivAutoencoder(15,[16,32],0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "90623821-0b3e-4c6d-a417-6f9a34832535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before encoder torch.Size([128, 15, 32, 32])\n",
      "torch.Size([128, 16, 32, 32])\n",
      "torch.Size([128, 16, 16, 16])\n",
      "torch.Size([128, 32, 16, 16])\n",
      "torch.Size([128, 32, 8, 8])\n",
      "before decoder torch.Size([128, 32, 4, 4])\n",
      "torch.Size([128, 256, 8, 8])\n",
      "torch.Size([128, 128, 16, 16])\n",
      "torch.Size([128, 128, 32, 32])\n",
      "torch.Size([128, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875dd69c-48e2-4c94-8923-1e104dbab502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
